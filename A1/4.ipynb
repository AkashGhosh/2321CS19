{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Kzip1QGWC74",
        "outputId": "9621e3af-6340-4954-930f-67a9eeca9406"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Informer2020'...\n",
            "remote: Enumerating objects: 576, done.\u001b[K\n",
            "remote: Counting objects: 100% (222/222), done.\u001b[K\n",
            "remote: Compressing objects: 100% (34/34), done.\u001b[K\n",
            "remote: Total 576 (delta 199), reused 188 (delta 188), pack-reused 354\u001b[K\n",
            "Receiving objects: 100% (576/576), 6.48 MiB | 16.79 MiB/s, done.\n",
            "Resolving deltas: 100% (336/336), done.\n",
            "Cloning into 'ETDataset'...\n",
            "remote: Enumerating objects: 187, done.\u001b[K\n",
            "remote: Counting objects: 100% (28/28), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 187 (delta 25), reused 20 (delta 20), pack-reused 159\u001b[K\n",
            "Receiving objects: 100% (187/187), 3.86 MiB | 20.46 MiB/s, done.\n",
            "Resolving deltas: 100% (62/62), done.\n",
            "ETDataset  Informer2020  sample_data\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/zhouhaoyi/Informer2020.git\n",
        "!git clone https://github.com/zhouhaoyi/ETDataset.git\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "if not 'Informer2020' in sys.path:\n",
        "    sys.path += ['Informer2020']"
      ],
      "metadata": {
        "id": "R5fnmQfXvKVt"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from utils.tools import dotdict\n",
        "from exp.exp_informer import Exp_Informer\n",
        "import torch"
      ],
      "metadata": {
        "id": "NSsWF4XTWklh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Experiments using ProbParse Attention\n"
      ],
      "metadata": {
        "id": "lZ3Uu9FHdtWd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "args = dotdict()\n",
        "\n",
        "args.model = 'informer' # model of experiment, options: [informer, informerstack, informerlight(TBD)]\n",
        "\n",
        "args.data = 'ETTh1' # data\n",
        "args.root_path = './ETDataset/ETT-small/' # root path of data file\n",
        "args.data_path = 'ETTh1.csv' # data file\n",
        "args.features = 'M' # forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate\n",
        "args.target = 'OT' # target feature in S or MS task\n",
        "args.freq = 'h' # freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h\n",
        "args.checkpoints = './informer_checkpoints' # location of model checkpoints\n",
        "\n",
        "args.seq_len = 96 # input sequence length of Informer encoder\n",
        "args.label_len = 48 # start token length of Informer decoder\n",
        "args.pred_len = 24 # prediction sequence length\n",
        "# Informer decoder input: concat[start token series(label_len), zero padding series(pred_len)]\n",
        "\n",
        "args.enc_in = 7 # encoder input size\n",
        "args.dec_in = 7 # decoder input size\n",
        "args.c_out = 7 # output size\n",
        "args.factor = 5 # probsparse attn factor\n",
        "args.d_model = 512 # dimension of model\n",
        "args.n_heads = 8 # num of heads\n",
        "args.e_layers = 2 # num of encoder layers\n",
        "args.d_layers = 1 # num of decoder layers\n",
        "args.d_ff = 2048 # dimension of fcn in model\n",
        "args.dropout = 0.05 # dropout\n",
        "args.attn = 'prob' # attention used in encoder, options:[prob, full]\n",
        "args.embed = 'timeF' # time features encoding, options:[timeF, fixed, learned]\n",
        "args.activation = 'gelu' # activation\n",
        "args.distil = True # whether to use distilling in encoder\n",
        "args.output_attention = False # whether to output attention in ecoder\n",
        "args.mix = True\n",
        "args.padding = 0\n",
        "args.freq = 'h'\n",
        "\n",
        "args.batch_size = 32\n",
        "args.learning_rate = 0.0001\n",
        "args.loss = 'mse'\n",
        "args.lradj = 'type1'\n",
        "args.use_amp = False # whether to use automatic mixed precision training\n",
        "\n",
        "args.num_workers = 0\n",
        "args.itr = 1\n",
        "args.train_epochs = 6\n",
        "args.patience = 3\n",
        "args.des = 'exp'\n",
        "\n",
        "args.use_gpu = True if torch.cuda.is_available() else False\n",
        "args.gpu = 0\n",
        "\n",
        "args.use_multi_gpu = False\n",
        "args.devices = '0,1,2,3'\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "9qM-K7Y6_H2m"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args.use_gpu = True if torch.cuda.is_available() and args.use_gpu else False\n",
        "\n",
        "if args.use_gpu and args.use_multi_gpu:\n",
        "    args.devices = args.devices.replace(' ','')\n",
        "    device_ids = args.devices.split(',')\n",
        "    args.device_ids = [int(id_) for id_ in device_ids]\n",
        "    args.gpu = args.device_ids[0]"
      ],
      "metadata": {
        "id": "B1eLGNblcHu0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set augments by using data name\n",
        "data_parser = {\n",
        "    'ETTh1':{'data':'ETTh1.csv','T':'OT','M':[7,7,7],'S':[1,1,1],'MS':[7,7,1]},\n",
        "    'ETTh2':{'data':'ETTh2.csv','T':'OT','M':[7,7,7],'S':[1,1,1],'MS':[7,7,1]},\n",
        "    'ETTm1':{'data':'ETTm1.csv','T':'OT','M':[7,7,7],'S':[1,1,1],'MS':[7,7,1]},\n",
        "    'ETTm2':{'data':'ETTm2.csv','T':'OT','M':[7,7,7],'S':[1,1,1],'MS':[7,7,1]},\n",
        "}\n",
        "if args.data in data_parser.keys():\n",
        "    data_info = data_parser[args.data]\n",
        "    args.data_path = data_info['data']\n",
        "    args.target = data_info['T']\n",
        "    args.enc_in, args.dec_in, args.c_out = data_info[args.features]"
      ],
      "metadata": {
        "id": "YX4qATp1cHx2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args.detail_freq = args.freq\n",
        "args.freq = args.freq[-1:]\n",
        "print('Args in experiment:')\n",
        "print(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SpwGqqxHcH05",
        "outputId": "bd0aa820-01ab-4b66-90cc-4be9c0f50ea0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Args in experiment:\n",
            "{'model': 'informer', 'data': 'ETTh1', 'root_path': './ETDataset/ETT-small/', 'data_path': 'ETTh1.csv', 'features': 'M', 'target': 'OT', 'freq': 'h', 'checkpoints': './informer_checkpoints', 'seq_len': 96, 'label_len': 48, 'pred_len': 24, 'enc_in': 7, 'dec_in': 7, 'c_out': 7, 'factor': 5, 'd_model': 512, 'n_heads': 8, 'e_layers': 2, 'd_layers': 1, 'd_ff': 2048, 'dropout': 0.05, 'attn': 'prob', 'embed': 'timeF', 'activation': 'gelu', 'distil': True, 'output_attention': False, 'mix': True, 'padding': 0, 'batch_size': 32, 'learning_rate': 0.0001, 'loss': 'mse', 'lradj': 'type1', 'use_amp': False, 'num_workers': 0, 'itr': 1, 'train_epochs': 6, 'patience': 3, 'des': 'exp', 'use_gpu': True, 'gpu': 0, 'use_multi_gpu': False, 'devices': '0,1,2,3', 'detail_freq': 'h'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Exp = Exp_Informer\n",
        "for ii in range(args.itr):\n",
        "    # setting record of experiments\n",
        "    setting = '{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_at{}_fc{}_eb{}_dt{}_mx{}_{}_{}'.format(args.model, args.data, args.features,\n",
        "                args.seq_len, args.label_len, args.pred_len,\n",
        "                args.d_model, args.n_heads, args.e_layers, args.d_layers, args.d_ff, args.attn, args.factor, args.embed, args.distil, args.mix, args.des, ii)\n",
        "\n",
        "    # set experiments\n",
        "    exp = Exp(args)\n",
        "\n",
        "    # train\n",
        "    print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
        "    exp.train(setting)\n",
        "\n",
        "    # test\n",
        "    print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
        "    exp.test(setting)\n",
        "\n",
        "    torch.cuda.empty_cache()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iX-aF6aVcH30",
        "outputId": "6b5ab502-a294-43b0-b59d-7ab430c2771b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use GPU: cuda:0\n",
            ">>>>>>>start training : informer_ETTh1_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "train 8521\n",
            "val 2857\n",
            "test 2857\n",
            "\titers: 100, epoch: 1 | loss: 0.4029384\n",
            "\tspeed: 0.1107s/iter; left time: 165.7398s\n",
            "\titers: 200, epoch: 1 | loss: 0.2936603\n",
            "\tspeed: 0.0232s/iter; left time: 32.4059s\n",
            "Epoch: 1 cost time: 14.567143201828003\n",
            "Epoch: 1, Steps: 266 | Train Loss: 0.4124441 Vali Loss: 0.6810287 Test Loss: 0.5705287\n",
            "Validation loss decreased (inf --> 0.681029).  Saving model ...\n",
            "Updating learning rate to 0.0001\n",
            "\titers: 100, epoch: 2 | loss: 0.2617782\n",
            "\tspeed: 0.0581s/iter; left time: 71.5813s\n",
            "\titers: 200, epoch: 2 | loss: 0.2564602\n",
            "\tspeed: 0.0231s/iter; left time: 26.0889s\n",
            "Epoch: 2 cost time: 6.128161430358887\n",
            "Epoch: 2, Steps: 266 | Train Loss: 0.2615401 Vali Loss: 0.6586866 Test Loss: 0.5706112\n",
            "Validation loss decreased (0.681029 --> 0.658687).  Saving model ...\n",
            "Updating learning rate to 5e-05\n",
            "\titers: 100, epoch: 3 | loss: 0.1945422\n",
            "\tspeed: 0.0585s/iter; left time: 56.4144s\n",
            "\titers: 200, epoch: 3 | loss: 0.1678759\n",
            "\tspeed: 0.0230s/iter; left time: 19.8965s\n",
            "Epoch: 3 cost time: 6.148276329040527\n",
            "Epoch: 3, Steps: 266 | Train Loss: 0.2047848 Vali Loss: 0.6980057 Test Loss: 0.6165588\n",
            "EarlyStopping counter: 1 out of 3\n",
            "Updating learning rate to 2.5e-05\n",
            "\titers: 100, epoch: 4 | loss: 0.1801352\n",
            "\tspeed: 0.0575s/iter; left time: 40.2245s\n",
            "\titers: 200, epoch: 4 | loss: 0.1661193\n",
            "\tspeed: 0.0228s/iter; left time: 13.6659s\n",
            "Epoch: 4 cost time: 6.126339673995972\n",
            "Epoch: 4, Steps: 266 | Train Loss: 0.1761686 Vali Loss: 0.7196397 Test Loss: 0.6346047\n",
            "EarlyStopping counter: 2 out of 3\n",
            "Updating learning rate to 1.25e-05\n",
            "\titers: 100, epoch: 5 | loss: 0.1478924\n",
            "\tspeed: 0.0573s/iter; left time: 24.7922s\n",
            "\titers: 200, epoch: 5 | loss: 0.1597521\n",
            "\tspeed: 0.0231s/iter; left time: 7.6767s\n",
            "Epoch: 5 cost time: 6.125267028808594\n",
            "Epoch: 5, Steps: 266 | Train Loss: 0.1626055 Vali Loss: 0.7303019 Test Loss: 0.6982811\n",
            "EarlyStopping counter: 3 out of 3\n",
            "Early stopping\n",
            ">>>>>>>testing : informer_ETTh1_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "test 2857\n",
            "test shape: (89, 32, 24, 7) (89, 32, 24, 7)\n",
            "test shape: (2848, 24, 7) (2848, 24, 7)\n",
            "mse:0.5705819725990295, mae:0.5595218539237976\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Experiments using Canonical Attention"
      ],
      "metadata": {
        "id": "CtctWFK0eCaW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##Experiments using Canonical Attention\n",
        "args = dotdict()\n",
        "\n",
        "args.model = 'informer' # model of experiment, options: [informer, informerstack, informerlight(TBD)]\n",
        "\n",
        "args.data = 'ETTh1' # data\n",
        "args.root_path = './ETDataset/ETT-small/' # root path of data file\n",
        "args.data_path = 'ETTh1.csv' # data file\n",
        "args.features = 'M' # forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate\n",
        "args.target = 'OT' # target feature in S or MS task\n",
        "args.freq = 'h' # freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h\n",
        "args.checkpoints = './informer_checkpoints' # location of model checkpoints\n",
        "\n",
        "args.seq_len = 96 # input sequence length of Informer encoder\n",
        "args.label_len = 48 # start token length of Informer decoder\n",
        "args.pred_len = 24 # prediction sequence length\n",
        "# Informer decoder input: concat[start token series(label_len), zero padding series(pred_len)]\n",
        "\n",
        "args.enc_in = 7 # encoder input size\n",
        "args.dec_in = 7 # decoder input size\n",
        "args.c_out = 7 # output size\n",
        "args.factor = 5 # probsparse attn factor\n",
        "args.d_model = 512 # dimension of model\n",
        "args.n_heads = 8 # num of heads\n",
        "args.e_layers = 2 # num of encoder layers\n",
        "args.d_layers = 1 # num of decoder layers\n",
        "args.d_ff = 2048 # dimension of fcn in model\n",
        "args.dropout = 0.05 # dropout\n",
        "args.attn = 'full' # attention used in encoder, options:[prob, full]\n",
        "args.embed = 'timeF' # time features encoding, options:[timeF, fixed, learned]\n",
        "args.activation = 'gelu' # activation\n",
        "args.distil = True # whether to use distilling in encoder\n",
        "args.output_attention = False # whether to output attention in ecoder\n",
        "args.mix = True\n",
        "args.padding = 0\n",
        "args.freq = 'h'\n",
        "\n",
        "args.batch_size = 32\n",
        "args.learning_rate = 0.0001\n",
        "args.loss = 'mse'\n",
        "args.lradj = 'type1'\n",
        "args.use_amp = False # whether to use automatic mixed precision training\n",
        "\n",
        "args.num_workers = 0\n",
        "args.itr = 1\n",
        "args.train_epochs = 6\n",
        "args.patience = 3\n",
        "args.des = 'exp'\n",
        "\n",
        "args.use_gpu = True if torch.cuda.is_available() else False\n",
        "args.gpu = 0\n",
        "\n",
        "args.use_multi_gpu = False\n",
        "args.devices = '0,1,2,3'\n"
      ],
      "metadata": {
        "id": "VDqrBXSlcH63"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args.use_gpu = True if torch.cuda.is_available() and args.use_gpu else False\n",
        "\n",
        "if args.use_gpu and args.use_multi_gpu:\n",
        "    args.devices = args.devices.replace(' ','')\n",
        "    device_ids = args.devices.split(',')\n",
        "    args.device_ids = [int(id_) for id_ in device_ids]\n",
        "    args.gpu = args.device_ids[0]"
      ],
      "metadata": {
        "id": "1Z8ovMr7cH9w"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set augments by using data name\n",
        "data_parser = {\n",
        "    'ETTh1':{'data':'ETTh1.csv','T':'OT','M':[7,7,7],'S':[1,1,1],'MS':[7,7,1]},\n",
        "    'ETTh2':{'data':'ETTh2.csv','T':'OT','M':[7,7,7],'S':[1,1,1],'MS':[7,7,1]},\n",
        "    'ETTm1':{'data':'ETTm1.csv','T':'OT','M':[7,7,7],'S':[1,1,1],'MS':[7,7,1]},\n",
        "    'ETTm2':{'data':'ETTm2.csv','T':'OT','M':[7,7,7],'S':[1,1,1],'MS':[7,7,1]},\n",
        "}\n",
        "if args.data in data_parser.keys():\n",
        "    data_info = data_parser[args.data]\n",
        "    args.data_path = data_info['data']\n",
        "    args.target = data_info['T']\n",
        "    args.enc_in, args.dec_in, args.c_out = data_info[args.features]"
      ],
      "metadata": {
        "id": "-OF_I9oEcIBJ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "args.detail_freq = args.freq\n",
        "args.freq = args.freq[-1:]\n",
        "print('Args in experiment:')\n",
        "print(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HE7TyiIaewBb",
        "outputId": "16732b34-7794-4318-d6aa-1f70f9ae36f8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Args in experiment:\n",
            "{'model': 'informer', 'data': 'ETTh1', 'root_path': './ETDataset/ETT-small/', 'data_path': 'ETTh1.csv', 'features': 'M', 'target': 'OT', 'freq': 'h', 'checkpoints': './informer_checkpoints', 'seq_len': 96, 'label_len': 48, 'pred_len': 24, 'enc_in': 7, 'dec_in': 7, 'c_out': 7, 'factor': 5, 'd_model': 512, 'n_heads': 8, 'e_layers': 2, 'd_layers': 1, 'd_ff': 2048, 'dropout': 0.05, 'attn': 'full', 'embed': 'timeF', 'activation': 'gelu', 'distil': True, 'output_attention': False, 'mix': True, 'padding': 0, 'batch_size': 32, 'learning_rate': 0.0001, 'loss': 'mse', 'lradj': 'type1', 'use_amp': False, 'num_workers': 0, 'itr': 1, 'train_epochs': 6, 'patience': 3, 'des': 'exp', 'use_gpu': True, 'gpu': 0, 'use_multi_gpu': False, 'devices': '0,1,2,3', 'detail_freq': 'h'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Exp = Exp_Informer\n",
        "for ii in range(args.itr):\n",
        "    # setting record of experiments\n",
        "    setting = '{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_at{}_fc{}_eb{}_dt{}_mx{}_{}_{}'.format(args.model, args.data, args.features,\n",
        "                args.seq_len, args.label_len, args.pred_len,\n",
        "                args.d_model, args.n_heads, args.e_layers, args.d_layers, args.d_ff, args.attn, args.factor, args.embed, args.distil, args.mix, args.des, ii)\n",
        "\n",
        "    # set experiments\n",
        "    exp = Exp(args)\n",
        "\n",
        "    # train\n",
        "    print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
        "    exp.train(setting)\n",
        "\n",
        "    # test\n",
        "    print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
        "    exp.test(setting)\n",
        "\n",
        "    torch.cuda.empty_cache()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9KU1Z6mewEy",
        "outputId": "e2d986d1-b165-4785-ecf8-f939ef61a8cc"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use GPU: cuda:0\n",
            ">>>>>>>start training : informer_ETTh1_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_atfull_fc5_ebtimeF_dtTrue_mxTrue_exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "train 8521\n",
            "val 2857\n",
            "test 2857\n",
            "\titers: 100, epoch: 1 | loss: 0.3704931\n",
            "\tspeed: 0.0168s/iter; left time: 25.1264s\n",
            "\titers: 200, epoch: 1 | loss: 0.2799826\n",
            "\tspeed: 0.0165s/iter; left time: 23.0333s\n",
            "Epoch: 1 cost time: 4.410358667373657\n",
            "Epoch: 1, Steps: 266 | Train Loss: 0.3732847 Vali Loss: 0.6176584 Test Loss: 0.5359716\n",
            "Validation loss decreased (inf --> 0.617658).  Saving model ...\n",
            "Updating learning rate to 0.0001\n",
            "\titers: 100, epoch: 2 | loss: 0.2562085\n",
            "\tspeed: 0.0424s/iter; left time: 52.1950s\n",
            "\titers: 200, epoch: 2 | loss: 0.2289660\n",
            "\tspeed: 0.0164s/iter; left time: 18.5533s\n",
            "Epoch: 2 cost time: 4.440632581710815\n",
            "Epoch: 2, Steps: 266 | Train Loss: 0.2318108 Vali Loss: 0.5949850 Test Loss: 0.4994430\n",
            "Validation loss decreased (0.617658 --> 0.594985).  Saving model ...\n",
            "Updating learning rate to 5e-05\n",
            "\titers: 100, epoch: 3 | loss: 0.1797017\n",
            "\tspeed: 0.0424s/iter; left time: 40.8860s\n",
            "\titers: 200, epoch: 3 | loss: 0.1577341\n",
            "\tspeed: 0.0164s/iter; left time: 14.1996s\n",
            "Epoch: 3 cost time: 4.374150514602661\n",
            "Epoch: 3, Steps: 266 | Train Loss: 0.1730727 Vali Loss: 0.6190338 Test Loss: 0.5685397\n",
            "EarlyStopping counter: 1 out of 3\n",
            "Updating learning rate to 2.5e-05\n",
            "\titers: 100, epoch: 4 | loss: 0.1619400\n",
            "\tspeed: 0.0411s/iter; left time: 28.7349s\n",
            "\titers: 200, epoch: 4 | loss: 0.1256611\n",
            "\tspeed: 0.0163s/iter; left time: 9.7461s\n",
            "Epoch: 4 cost time: 4.362512588500977\n",
            "Epoch: 4, Steps: 266 | Train Loss: 0.1470253 Vali Loss: 0.6808723 Test Loss: 0.5950612\n",
            "EarlyStopping counter: 2 out of 3\n",
            "Updating learning rate to 1.25e-05\n",
            "\titers: 100, epoch: 5 | loss: 0.1200662\n",
            "\tspeed: 0.0412s/iter; left time: 17.8183s\n",
            "\titers: 200, epoch: 5 | loss: 0.1364605\n",
            "\tspeed: 0.0166s/iter; left time: 5.5309s\n",
            "Epoch: 5 cost time: 4.4190895557403564\n",
            "Epoch: 5, Steps: 266 | Train Loss: 0.1350727 Vali Loss: 0.6779082 Test Loss: 0.6195741\n",
            "EarlyStopping counter: 3 out of 3\n",
            "Early stopping\n",
            ">>>>>>>testing : informer_ETTh1_ftM_sl96_ll48_pl24_dm512_nh8_el2_dl1_df2048_atfull_fc5_ebtimeF_dtTrue_mxTrue_exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "test 2857\n",
            "test shape: (89, 32, 24, 7) (89, 32, 24, 7)\n",
            "test shape: (2848, 24, 7) (2848, 24, 7)\n",
            "mse:0.4994429349899292, mae:0.5118056535720825\n"
          ]
        }
      ]
    }
  ]
}